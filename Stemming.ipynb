{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b29ce348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5033463",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "def56a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.', 'Stemming is important in natural language understanding (NLU) and natural language processing (NLP).']\n",
      "['Stemming', 'is', 'the', 'process', 'of', 'reducing', 'a', 'word', 'to', 'its', 'word', 'stem', 'that', 'affixes', 'to', 'suffixes', 'and', 'prefixes', 'or', 'to', 'the', 'roots', 'of', 'words', 'known', 'as', 'a', 'lemma', '.', 'Stemming', 'is', 'important', 'in', 'natural', 'language', 'understanding', '(', 'NLU', ')', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "sent=sent_tokenize(corpus)\n",
    "words=word_tokenize(corpus)\n",
    "print(sent)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38b0377b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00452387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemming', 'is', 'the', 'process', 'of', 'reducing', 'a', 'word', 'to', 'its', 'word', 'stem', 'that', 'affixes', 'to', 'suffixes', 'and', 'prefixes', 'or', 'to', 'the', 'roots', 'of', 'words', 'known', 'as', 'a', 'lemma', '.']\n",
      "['Stemming', 'is', 'important', 'in', 'natural', 'language', 'understanding', '(', 'NLU', ')', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "for l in sent:\n",
    "    print(word_tokenize(l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5c9e7",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming -- PorterStemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1db2715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The i after stemming is ----> i\n",
      "The am after stemming is ----> am\n",
      "The eating after stemming is ----> eat\n",
      "The and after stemming is ----> and\n",
      "The walking after stemming is ----> walk\n",
      "The . after stemming is ----> .\n",
      "The i after stemming is ----> i\n",
      "The would after stemming is ----> would\n",
      "The like after stemming is ----> like\n",
      "The to after stemming is ----> to\n",
      "The take after stemming is ----> take\n",
      "The some after stemming is ----> some\n",
      "The rest after stemming is ----> rest\n",
      "The and after stemming is ----> and\n",
      "The have after stemming is ----> have\n",
      "The fun after stemming is ----> fun\n",
      "The watching after stemming is ----> watch\n",
      "The movies after stemming is ----> movi\n"
     ]
    }
   ],
   "source": [
    "data=\"\"\"I am eating and walking . I would like to take some rest and have fun watching movies\"\"\"\n",
    "data=data.lower()\n",
    "w=word_tokenize(data)\n",
    "for l in w:\n",
    "    output=stemmer.stem(l)\n",
    "    print(f\"The {l} after stemming is ---->\",output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503ec7c",
   "metadata": {},
   "source": [
    "### Reg Exp Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stem1=RegexpStemmer('ing$|s$|e$|able$|ly$', min=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fedb937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I   ---> I\n",
      " am   ---> am\n",
      " able   ---> \n",
      " to   ---> to\n",
      " do   ---> do\n",
      " whatever   ---> whatever\n",
      " i   ---> i\n",
      " can   ---> can\n",
      " till   ---> till\n",
      " i   ---> i\n",
      " breathe   ---> breath\n",
      " but   ---> but\n",
      " my   ---> my\n",
      " family   ---> fami\n",
      " encourages   ---> encourage\n",
      " me   ---> me\n",
      " to   ---> to\n",
      " limit   ---> limit\n",
      " my   ---> my\n",
      " abilities   ---> abilitie\n",
      " and   ---> and\n",
      " start   ---> start\n",
      " reading   ---> read\n"
     ]
    }
   ],
   "source": [
    "d=\"\"\"I am able to do whatever i can till i breathe but my family encourages me to limit my abilities and start reading\"\"\"\n",
    "w=word_tokenize(d)\n",
    "for l in w:\n",
    "    output1=stem1.stem(l)\n",
    "    print(f\" {l}   --->\", output1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893d5e4",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stem2=SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68ddabb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I   ---> i\n",
      " am   ---> am\n",
      " able   ---> abl\n",
      " to   ---> to\n",
      " do   ---> do\n",
      " whatever   ---> whatev\n",
      " i   ---> i\n",
      " can   ---> can\n",
      " till   ---> till\n",
      " i   ---> i\n",
      " breathe   ---> breath\n",
      " but   ---> but\n",
      " my   ---> my\n",
      " family   ---> famili\n",
      " encourages   ---> encourag\n",
      " me   ---> me\n",
      " to   ---> to\n",
      " limit   ---> limit\n",
      " my   ---> my\n",
      " abilities   ---> abil\n",
      " and   ---> and\n",
      " start   ---> start\n",
      " reading   ---> read\n"
     ]
    }
   ],
   "source": [
    "d=\"\"\"I am able to do whatever i can till i breathe but my family encourages me to limit my abilities and start reading\"\"\"\n",
    "w=word_tokenize(d)\n",
    "for l in w:\n",
    "    output1=stem2.stem(l)\n",
    "    print(f\" {l}   --->\", output1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
